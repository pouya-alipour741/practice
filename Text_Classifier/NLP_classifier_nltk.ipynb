{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ec0a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e545f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08fb98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_Reviews = 'https://raw.githubusercontent.com/futurexskill/ml-model-deployment/main/Restaurant_Reviews.tsv.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4cc5715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(Restaurant_Reviews, delimiter= '\\t', quoting = 3)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b905748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk = pd.read_csv(Restaurant_Reviews, delimiter= '\\t', chunksize=100)\n",
    "# dataset['Review'] = dataset['Review'].factorize()[0]\n",
    "# # for i in chunk:\n",
    "# #     for row in i.itertuples():\n",
    "# #         print(row[\"Review\"])\n",
    "# dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706e26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00b6892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import IterableDataset\n",
    "# class IterableCustomDataset(IterableDataset):\n",
    "#     def __init__(self, data_path):\n",
    "#         self.data_path = data_path\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         for chunk in pd.read_csv(Restaurant_Reviews, delimiter= '\\t', chunksize=100):\n",
    "#             for row in chunk.itertuples():\n",
    "#                 yield (row[\"Review\"], row[\"Liked\"])\n",
    "            \n",
    "# data = IterableCustomDataset(Restaurant_Reviews)   \n",
    "# for x, y in data:\n",
    "#     print(y, \"-\"*10, x)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f2519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5865bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d75b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  1000 non-null   object\n",
      " 1   Liked   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ce59af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f652ff6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c0966b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow love place'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    customer_review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    customer_review = customer_review.lower()\n",
    "    customer_review = customer_review.split()\n",
    "    clean_review = [ps.stem(word) for word in customer_review if not word in stopwords.words('english') ]\n",
    "    clean_review = \" \".join(clean_review)\n",
    "    corpus.append(clean_review)\n",
    "corpus\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "737b978b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4019a01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cashier care ever say still end wayyy overpr'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c9a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a62876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.40419648, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.37355828, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.36223438, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.44213051,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.26570498, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.38716092, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.38716092, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2500, min_df=3, max_df=0.6)\n",
    "x = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "x[999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8cce541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 467)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape #1000 sentences and 467 words(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be502699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.iloc[:, 1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54f2cd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "533f921f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "47e9db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train).float()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b39b33ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 467])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2fd064f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=x_train.shape[1]\n",
    "output_size=2\n",
    "hidden_size=500\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44009449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model = NET()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6497c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 467])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dfd18c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "539715be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouya\\AppData\\Local\\Temp\\ipykernel_115384\\2515444589.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 0.6938283443450928\n",
      "Epoch 20 loss 0.02992674894630909\n",
      "Epoch 40 loss 0.027662253007292747\n",
      "Epoch 60 loss 0.027286969125270844\n",
      "Epoch 80 loss 0.027784429490566254\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 ==0:\n",
    "        print('Epoch',epoch, 'loss',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "298f4326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [\"Good batting by England\"]\n",
    "sample = vectorizer.transform(sample).toarray()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "edf9d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.from_numpy(sample).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "59acaa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouya\\AppData\\Local\\Temp\\ipykernel_115384\\2515444589.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7325, -0.6553]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = model(sample)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39ebb7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2 = [\"bad performance by India in the match\"]\n",
    "sample2 = vectorizer.transform(sample2).toarray()\n",
    "sample2 = torch.from_numpy(sample2).float()\n",
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8ddb8da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouya\\AppData\\Local\\Temp\\ipykernel_115384\\2515444589.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000, -61.6875]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f209bf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.1534,  0.1109, -0.1540,  ...,  0.0190,  0.1267, -0.1386],\n",
       "                      [-0.0455, -0.0013, -0.0237,  ...,  0.1780, -0.0101, -0.0449],\n",
       "                      [ 0.0710, -0.0438, -0.1039,  ..., -0.0597, -0.0482, -0.0061],\n",
       "                      ...,\n",
       "                      [-0.0282,  0.1272,  0.1664,  ...,  0.0859, -0.0583,  0.1706],\n",
       "                      [ 0.1191,  0.0828, -0.0993,  ..., -0.0136,  0.0824, -0.0716],\n",
       "                      [-0.0900,  0.0714,  0.0751,  ...,  0.0211,  0.0402,  0.1495]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 4.4158e-02, -1.1699e-01,  2.1638e-03, -9.4221e-03, -4.0179e-02,\n",
       "                       4.3956e-02,  1.5136e-02,  4.7619e-02, -4.1465e-02,  3.0815e-02,\n",
       "                      -4.2394e-02, -2.3347e-02,  1.2458e-02, -1.3419e-02, -2.0625e-02,\n",
       "                       1.0117e-02, -1.2389e-02,  1.7930e-02,  1.7348e-02, -2.3828e-02,\n",
       "                      -5.8126e-02, -3.3375e-03,  1.7586e-02, -9.8042e-02, -3.8068e-02,\n",
       "                      -5.0679e-02, -4.9060e-02, -4.8556e-02, -1.9868e-02,  1.1023e-02,\n",
       "                      -1.0093e-01, -2.3738e-02, -2.1479e-02, -7.1628e-02, -9.7555e-03,\n",
       "                      -2.7723e-02, -8.0107e-03,  3.2626e-02,  2.8047e-02, -4.0687e-02,\n",
       "                      -2.7344e-02, -1.9757e-02,  3.1606e-02, -6.1195e-02, -3.2034e-02,\n",
       "                       5.3313e-02, -6.0012e-02, -1.4395e-02,  2.1176e-02,  2.6814e-02,\n",
       "                       1.8578e-02,  5.9397e-03,  2.9662e-02, -1.0043e-01,  1.0527e-02,\n",
       "                      -4.2557e-03, -6.8603e-03, -1.4684e-02, -8.4825e-02,  3.5690e-02,\n",
       "                       1.3845e-02, -1.1524e-02,  8.2773e-03, -2.0305e-02, -1.1366e-01,\n",
       "                       4.3214e-03, -1.3931e-01, -1.1046e-02,  3.5314e-02, -9.1084e-02,\n",
       "                      -3.3080e-02, -1.3135e-01,  4.8017e-02, -5.5063e-02, -9.2474e-02,\n",
       "                      -6.0643e-02, -4.2685e-02, -5.1963e-02, -7.1487e-03, -5.1555e-02,\n",
       "                      -2.7907e-02, -1.0634e-02, -7.1122e-02, -6.1699e-02, -6.9152e-02,\n",
       "                       3.9203e-03, -5.4166e-02, -4.9967e-02,  4.4431e-02,  1.0971e-02,\n",
       "                      -5.7687e-02,  3.0100e-02,  4.2568e-02, -3.8547e-02,  1.0894e-02,\n",
       "                       3.7805e-03, -4.2992e-02,  2.3764e-02, -3.6782e-02,  2.2158e-02,\n",
       "                       3.4885e-02, -4.4463e-02, -1.1642e-01,  1.1355e-02,  1.9710e-02,\n",
       "                      -2.6990e-02, -3.1728e-02,  5.3613e-03,  2.0683e-02, -1.9767e-02,\n",
       "                      -4.5591e-03, -9.6114e-02, -7.9416e-02, -6.9368e-02, -9.4459e-02,\n",
       "                       1.7393e-02, -2.0497e-02, -2.6851e-02, -1.0333e-01,  3.3240e-02,\n",
       "                      -6.0875e-02,  2.7099e-02, -3.2666e-02,  2.1380e-03, -2.8756e-02,\n",
       "                      -2.2167e-02, -7.8181e-02, -3.2429e-02,  4.5231e-03,  3.0667e-03,\n",
       "                       2.0513e-02, -4.5468e-02, -8.2147e-03, -8.2537e-02,  4.4242e-02,\n",
       "                      -2.6389e-02, -4.5241e-02, -8.4767e-03,  2.6236e-02, -3.6617e-02,\n",
       "                      -3.3462e-02,  1.3085e-02,  6.5567e-03,  2.4470e-02, -9.7474e-02,\n",
       "                      -5.0172e-02,  4.2429e-02,  5.2252e-02, -5.9970e-02,  2.9787e-02,\n",
       "                       1.1082e-02, -7.3342e-03, -9.1001e-02,  4.1844e-02, -4.5838e-02,\n",
       "                       4.2624e-02, -1.9014e-02,  3.5593e-02, -3.7359e-02, -5.5896e-02,\n",
       "                       1.5907e-02, -1.0410e-01,  3.0519e-02,  3.9875e-02, -2.6350e-02,\n",
       "                      -7.7987e-03, -5.4552e-02, -1.5220e-02, -3.0317e-02, -1.6916e-02,\n",
       "                       2.1329e-02,  2.3768e-02, -8.6036e-03,  1.7280e-02, -3.1209e-02,\n",
       "                      -3.5935e-03,  3.2078e-02, -8.5283e-02,  2.6445e-02, -9.4527e-03,\n",
       "                      -1.0341e-01, -4.8694e-03,  2.2761e-02, -2.5393e-02, -7.7864e-03,\n",
       "                      -1.8016e-02,  3.1780e-02,  1.8326e-02, -1.7771e-02, -3.1852e-02,\n",
       "                      -9.3624e-02,  1.5976e-02,  2.9601e-03, -3.4361e-02,  1.7631e-02,\n",
       "                       2.7270e-02,  4.0001e-03,  1.4022e-02, -1.6953e-02, -1.5638e-02,\n",
       "                      -3.6366e-02,  2.3365e-02,  2.1517e-02,  4.4434e-04,  2.3870e-02,\n",
       "                      -1.0329e-02, -1.0726e-01, -3.0126e-03, -7.8436e-02, -3.0530e-02,\n",
       "                      -7.9433e-03, -2.9861e-02, -1.5635e-02, -2.5542e-02, -2.2378e-02,\n",
       "                       8.9435e-03, -1.9506e-02,  1.7555e-03, -2.0008e-02, -1.1858e-02,\n",
       "                      -1.3101e-02, -1.1887e-02, -1.8305e-03,  2.1407e-02, -3.8712e-02,\n",
       "                      -4.4317e-02,  1.0283e-02, -1.1618e-02, -7.5120e-02, -2.4808e-02,\n",
       "                       7.0393e-03, -1.2199e-02, -3.0133e-02, -3.0386e-02, -1.9430e-02,\n",
       "                       1.4413e-02,  3.8341e-02, -5.1450e-02,  3.7829e-02,  5.1135e-02,\n",
       "                      -7.2597e-02, -8.3484e-03,  3.9818e-03,  1.7930e-02, -9.1890e-03,\n",
       "                      -1.0031e-02, -9.5517e-03,  1.9746e-02,  1.6964e-02,  2.2494e-02,\n",
       "                       1.1468e-02, -2.6873e-02,  2.9626e-02,  2.4778e-02,  1.3048e-02,\n",
       "                      -8.3711e-03, -1.2575e-02,  6.1849e-02,  1.7342e-02, -4.4803e-02,\n",
       "                       1.4942e-02, -2.4034e-02,  2.6331e-02, -9.7574e-02,  3.0189e-02,\n",
       "                      -2.3713e-03, -1.0845e-03, -6.1203e-03,  3.9734e-03,  6.8532e-03,\n",
       "                      -2.0048e-02, -4.0008e-02,  4.3566e-02,  3.4671e-02,  5.1510e-02,\n",
       "                       9.4017e-04,  2.1270e-02, -1.4578e-02,  1.3016e-02, -5.8012e-02,\n",
       "                      -2.6204e-02, -4.7580e-02, -6.7441e-02, -2.8456e-02, -2.4554e-02,\n",
       "                       5.9290e-02,  1.0815e-02,  3.0921e-02,  4.9874e-02, -2.5222e-02,\n",
       "                      -5.4149e-03,  2.6510e-02,  4.4985e-02, -2.8965e-02, -1.8161e-03,\n",
       "                      -6.4964e-02, -5.7744e-02, -2.4686e-02,  1.9411e-02, -4.1397e-02,\n",
       "                      -7.3517e-03,  2.3728e-02,  2.9080e-02, -1.5899e-02, -1.4649e-02,\n",
       "                      -3.5538e-02, -7.2659e-02, -4.9325e-02, -2.6299e-02, -7.8491e-03,\n",
       "                      -3.4787e-02,  8.5037e-03, -3.6181e-02, -1.0079e-01,  5.4308e-03,\n",
       "                       9.8761e-03,  6.9375e-05,  2.1542e-02, -1.0192e-01,  1.3643e-02,\n",
       "                      -4.1184e-02, -4.4146e-02, -7.5236e-02, -6.2615e-03, -1.1236e-02,\n",
       "                      -1.1702e-02, -1.2120e-02, -1.0323e-01, -5.6556e-02,  2.1687e-02,\n",
       "                       8.5713e-03,  2.9086e-03, -2.7358e-02, -8.0972e-02, -2.6330e-02,\n",
       "                       1.5429e-02, -3.5181e-02, -1.4328e-02,  2.3770e-03,  3.3255e-02,\n",
       "                      -1.7267e-02, -5.3069e-02, -4.1912e-02,  1.6615e-02,  1.3208e-02,\n",
       "                      -9.3027e-02, -1.3933e-02, -4.9580e-02, -1.8081e-02,  1.6543e-02,\n",
       "                      -2.0996e-02, -1.7352e-02,  8.4956e-03, -2.6307e-02,  4.1867e-02,\n",
       "                       1.1428e-02,  2.9262e-02, -9.0480e-02, -5.0165e-03, -9.3907e-03,\n",
       "                      -5.4608e-03, -1.3146e-01,  3.0480e-03, -5.2860e-02, -8.9992e-02,\n",
       "                      -2.5096e-02,  2.0347e-02, -2.6678e-02,  3.1466e-02, -7.0298e-02,\n",
       "                      -4.6297e-02,  2.0670e-02, -2.3962e-02,  3.5925e-02, -5.2171e-02,\n",
       "                       1.5546e-02, -8.1681e-02,  2.3838e-02, -8.0895e-03, -3.8971e-02,\n",
       "                       6.2974e-02, -6.6578e-02,  1.9614e-02,  1.6251e-03, -2.1772e-02,\n",
       "                      -8.9375e-03, -7.1325e-02,  1.1691e-02, -4.5449e-02,  1.8056e-02,\n",
       "                       1.6215e-02, -1.8145e-02,  3.4032e-02, -1.8399e-02, -2.0628e-02,\n",
       "                       5.3226e-02, -1.6966e-03, -3.1701e-02, -4.9995e-02, -1.0625e-01,\n",
       "                      -6.6533e-03, -9.5481e-02,  2.9535e-02, -1.6785e-02,  1.7924e-03,\n",
       "                      -3.7243e-02, -2.6869e-02, -9.5043e-02,  8.0221e-02, -1.4099e-02,\n",
       "                      -1.8351e-02,  3.9491e-02, -8.6347e-02, -5.4950e-03,  1.8769e-02,\n",
       "                      -2.5026e-02,  6.1144e-03,  6.1037e-02, -2.6103e-02,  2.0045e-02,\n",
       "                       1.7299e-02,  6.0671e-03, -2.6791e-02, -3.5212e-02, -1.3251e-02,\n",
       "                      -6.2831e-02, -6.8560e-02, -3.1664e-03,  2.2598e-02, -2.1960e-02,\n",
       "                       2.8328e-02, -3.5764e-02, -4.1583e-03, -1.9099e-02, -2.9893e-02,\n",
       "                       2.7612e-02,  1.8381e-02,  3.7471e-02,  1.5150e-03, -5.0835e-02,\n",
       "                      -2.7925e-02, -1.2317e-02, -1.8863e-02,  2.5387e-02,  1.6507e-02,\n",
       "                      -7.0072e-02,  3.4864e-02, -1.0789e-01,  2.3150e-03,  7.7435e-03,\n",
       "                      -1.8701e-02,  1.0761e-02,  2.4442e-02, -1.1775e-02,  1.9817e-03,\n",
       "                       3.2356e-02, -4.0065e-02, -9.6739e-02, -7.5147e-02, -4.7101e-02,\n",
       "                      -6.9959e-03,  9.1168e-03, -5.0203e-03, -9.4745e-03,  8.2893e-03,\n",
       "                       3.9595e-03, -2.6882e-02, -7.6207e-02,  2.5476e-03, -1.0052e-01,\n",
       "                      -3.3599e-02, -3.7777e-02,  3.1178e-02, -3.5306e-02, -2.9125e-02,\n",
       "                      -1.1320e-01, -2.4045e-02,  1.0775e-02, -4.1282e-02, -1.1406e-01,\n",
       "                      -1.4540e-02, -2.9271e-02,  2.0665e-03,  8.0351e-03,  1.9883e-02,\n",
       "                      -1.1837e-01,  2.6348e-02, -3.0963e-03, -3.4076e-02,  1.3290e-02,\n",
       "                       2.9101e-02,  9.3826e-03,  3.1005e-02, -2.8986e-02, -2.9406e-02,\n",
       "                      -4.8806e-02, -8.6085e-02, -3.7473e-02,  1.1880e-02,  2.7476e-03])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0499,  0.0573, -0.0495,  ..., -0.0809, -0.0716,  0.0414],\n",
       "                      [-0.0985, -0.0361, -0.0533,  ...,  0.0923, -0.0681,  0.1227],\n",
       "                      [-0.0373, -0.0320, -0.0187,  ..., -0.0075,  0.0221, -0.0216],\n",
       "                      ...,\n",
       "                      [-0.0584, -0.0690, -0.0872,  ...,  0.0231,  0.0605, -0.0554],\n",
       "                      [-0.0870, -0.0676, -0.0864,  ..., -0.0969,  0.0829, -0.0545],\n",
       "                      [ 0.1419,  0.0958,  0.1065,  ..., -0.0133,  0.0834, -0.0983]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-8.1582e-02,  5.9694e-02, -9.1071e-02, -9.9690e-02, -7.7613e-02,\n",
       "                      -5.3021e-02, -3.5126e-02,  1.0440e-01, -7.0605e-02, -8.7985e-02,\n",
       "                      -5.2229e-03, -6.3189e-02, -7.7572e-02, -3.7475e-02,  7.1778e-02,\n",
       "                      -3.8005e-02, -1.1758e-02,  3.6737e-02, -1.0108e-01,  9.6902e-03,\n",
       "                       1.6516e-02, -4.0134e-02, -3.0904e-03,  7.4722e-02,  5.0697e-02,\n",
       "                       4.7591e-02, -6.4333e-02,  3.6674e-02, -9.5618e-02,  6.2818e-02,\n",
       "                       2.2021e-02, -8.7401e-02,  5.6008e-02,  5.3876e-02,  3.1983e-02,\n",
       "                      -6.2916e-02, -3.0146e-02,  6.0906e-02,  5.4160e-02, -9.3708e-02,\n",
       "                       1.7364e-02,  4.4371e-02,  5.9755e-02,  3.3773e-02, -4.7203e-02,\n",
       "                      -6.9505e-02, -7.9317e-02, -7.2375e-02,  1.2430e-02, -5.0991e-02,\n",
       "                       7.5774e-02, -8.3160e-02, -8.9028e-02,  8.1762e-02, -9.2927e-02,\n",
       "                       3.4218e-02,  5.0365e-02, -7.7969e-02, -3.3097e-02,  5.5508e-02,\n",
       "                       3.2963e-02, -6.5327e-02, -6.5527e-02,  6.9346e-02, -2.2865e-02,\n",
       "                      -5.6828e-02, -4.7097e-02,  5.3459e-02,  6.4311e-02, -9.8988e-02,\n",
       "                      -2.5557e-02, -9.6517e-02,  5.2449e-02,  2.7715e-02, -4.0393e-02,\n",
       "                      -6.6551e-02, -8.3164e-02,  1.5263e-02, -7.7890e-02,  6.1169e-02,\n",
       "                      -7.2589e-02, -7.3344e-02, -9.1664e-02,  3.7266e-02,  1.0542e-01,\n",
       "                       1.9990e-02,  3.0785e-02, -2.2704e-03, -3.8084e-02, -7.2790e-02,\n",
       "                       6.0140e-02,  4.8279e-02, -6.9328e-02, -6.2935e-02,  8.3111e-02,\n",
       "                       1.8593e-02, -9.0126e-02, -3.4347e-03, -5.9571e-02,  6.0865e-02,\n",
       "                      -6.4175e-02, -1.0438e-01, -3.3594e-02,  6.1891e-02,  5.2498e-02,\n",
       "                       3.8152e-02, -6.9917e-02, -8.0730e-02,  3.7229e-02, -5.1109e-02,\n",
       "                      -1.6940e-02, -7.5444e-02, -7.1505e-02,  7.2874e-02, -1.0033e-01,\n",
       "                      -1.1181e-02, -7.1048e-02,  6.4695e-02,  8.6802e-02,  1.9550e-02,\n",
       "                       3.5007e-02, -6.5616e-02,  6.9575e-02,  4.8275e-02, -2.6526e-02,\n",
       "                      -6.5933e-02,  3.4621e-02,  7.7637e-02,  6.1510e-02, -9.9543e-02,\n",
       "                       4.7941e-02,  3.7533e-02,  4.6812e-02,  4.5731e-02,  4.4317e-02,\n",
       "                       8.3722e-02, -4.9709e-02, -8.0842e-02, -8.8282e-02,  5.3190e-02,\n",
       "                      -7.6073e-02,  5.6142e-02, -9.2314e-02,  6.1767e-02,  5.0573e-03,\n",
       "                      -4.8116e-02,  5.6399e-03, -8.2567e-02,  5.2264e-02,  3.5107e-02,\n",
       "                       7.5075e-02,  3.0751e-02,  5.4681e-02, -2.7934e-02, -3.5833e-02,\n",
       "                       6.0347e-02,  1.0287e-01, -4.1561e-02,  5.2488e-02, -5.2044e-02,\n",
       "                       4.4444e-02,  4.0192e-02, -1.1147e-03,  5.2629e-02, -9.8077e-02,\n",
       "                      -2.5853e-02, -4.2222e-02, -9.5975e-02,  4.7707e-02, -7.1773e-02,\n",
       "                      -7.6266e-02,  5.7928e-02,  7.6801e-02,  4.5946e-02, -6.7691e-02,\n",
       "                      -6.1841e-02,  6.9703e-03,  1.2516e-03, -4.5036e-02,  7.6151e-02,\n",
       "                       6.9012e-02,  4.6105e-02,  3.0325e-02, -7.5313e-02,  9.0623e-02,\n",
       "                       6.2784e-02, -3.2404e-02,  8.6789e-02,  1.0828e-01,  3.6535e-02,\n",
       "                      -6.5066e-02,  1.0159e-02, -8.2319e-02,  5.9764e-02,  4.6715e-02,\n",
       "                       4.7612e-02,  4.8941e-02, -6.5493e-02,  3.9664e-02, -8.2250e-02,\n",
       "                       4.3052e-02,  4.3093e-02,  4.3653e-02,  3.7515e-02, -1.0647e-01,\n",
       "                       1.0728e-01, -5.0717e-02,  9.2974e-02,  3.1461e-02, -6.3254e-02,\n",
       "                       1.4881e-02, -5.5955e-02,  7.7569e-02, -4.4018e-02,  6.7035e-03,\n",
       "                       3.1781e-02, -3.1209e-02,  4.7933e-02,  5.7496e-02,  4.0575e-02,\n",
       "                      -8.2143e-02, -6.4089e-02, -7.5308e-02, -3.4362e-02,  2.1592e-02,\n",
       "                      -6.7015e-02, -7.4308e-02, -7.3557e-02,  8.3225e-02, -9.2348e-02,\n",
       "                       1.0941e-01,  8.4250e-02, -6.2948e-02,  3.8105e-02, -7.7660e-02,\n",
       "                       6.4356e-02,  4.1491e-02, -5.4864e-02,  4.8851e-02, -8.8816e-02,\n",
       "                       5.4244e-02,  3.1220e-03,  3.7009e-02,  2.2408e-02,  1.6753e-03,\n",
       "                       7.6393e-03, -3.9730e-02, -9.5482e-02,  5.5477e-02,  5.7416e-02,\n",
       "                       6.0808e-02, -7.3334e-02, -8.9866e-02, -7.6993e-02,  5.9048e-02,\n",
       "                       6.1743e-02, -6.7765e-02,  1.0958e-01,  6.0169e-02,  6.0033e-02,\n",
       "                       3.0473e-02,  6.0075e-02, -7.4225e-02,  4.4163e-02,  5.9556e-02,\n",
       "                      -6.6923e-02,  4.0097e-02,  1.3896e-02, -8.9585e-02,  1.5963e-03,\n",
       "                      -7.0346e-02, -5.1081e-02, -3.2770e-02, -6.4206e-02, -5.3386e-02,\n",
       "                       1.6972e-02, -4.2309e-02,  5.6382e-02,  6.7507e-02,  3.3638e-02,\n",
       "                       3.7178e-02, -3.2413e-02, -5.7130e-02, -6.7646e-02,  5.9664e-02,\n",
       "                       2.9842e-02, -4.6638e-02,  9.9272e-03, -4.8688e-02,  1.7293e-02,\n",
       "                       2.9461e-02,  5.8358e-02,  7.8845e-02, -6.3065e-02, -5.5931e-02,\n",
       "                      -8.4356e-02, -8.1521e-02, -7.3349e-02,  2.2459e-02, -7.7688e-02,\n",
       "                       3.9274e-02,  6.1694e-02,  4.1816e-02,  4.3730e-02, -7.4584e-02,\n",
       "                      -9.3873e-02, -8.8777e-02,  9.3390e-02, -5.9160e-02, -1.1414e-03,\n",
       "                       3.5299e-02, -6.3100e-02, -6.4527e-02, -1.1688e-02, -3.3464e-02,\n",
       "                       6.0525e-02, -6.9392e-02,  6.6172e-02,  6.6529e-02, -3.1056e-02,\n",
       "                       4.7924e-02, -3.1847e-02,  2.8987e-02,  4.9874e-02,  5.0892e-02,\n",
       "                       5.0339e-02, -6.1350e-02,  3.3938e-02,  4.9344e-02, -1.0394e-02,\n",
       "                      -8.1009e-02, -2.3067e-02,  5.7414e-02,  4.9522e-02,  6.2428e-02,\n",
       "                      -8.1928e-02, -4.8335e-02,  4.7667e-02,  4.5168e-02, -7.9217e-02,\n",
       "                      -5.9477e-02, -7.9832e-02, -5.6231e-02,  7.8721e-03, -7.2045e-02,\n",
       "                      -7.5562e-02, -8.3511e-02,  3.7022e-02,  4.9677e-02,  1.2955e-03,\n",
       "                      -7.3180e-02, -6.7378e-02,  8.4671e-02,  3.5345e-02,  6.6898e-02,\n",
       "                       5.0352e-02, -8.7890e-02, -4.5836e-02,  6.0241e-02, -1.4378e-02,\n",
       "                      -5.8885e-04,  3.4605e-02,  5.9758e-02,  4.7594e-02,  4.3621e-02,\n",
       "                      -8.5378e-02, -7.7548e-02, -2.4644e-02,  6.6716e-02, -2.9289e-02,\n",
       "                       2.5069e-02, -7.1504e-02,  5.7996e-02, -7.9119e-02,  6.2927e-02,\n",
       "                       2.1130e-02,  9.5958e-02, -4.3948e-02, -5.6747e-02,  6.4388e-02,\n",
       "                      -5.7999e-02, -6.5487e-02,  5.1722e-02,  7.6090e-02,  7.5342e-02,\n",
       "                       4.7661e-06, -3.4578e-02,  1.9032e-02, -3.7679e-02,  4.3554e-02,\n",
       "                       4.7069e-02, -2.5712e-02, -5.4581e-02,  5.3810e-02, -6.6751e-02,\n",
       "                      -5.2624e-02, -4.3022e-02,  9.7029e-02, -3.6262e-02,  5.2044e-02,\n",
       "                       6.0226e-02,  3.9042e-02, -6.1866e-02,  4.3240e-02, -7.5862e-02,\n",
       "                       1.0379e-02,  4.4396e-02,  1.1415e-01, -8.2799e-02, -7.2670e-02,\n",
       "                       4.5900e-02, -7.9329e-02,  7.6536e-02, -3.0140e-02, -5.7799e-02,\n",
       "                       5.3893e-02,  2.8914e-02,  1.5167e-02, -2.6815e-02,  2.7930e-02,\n",
       "                      -6.5953e-02,  1.4265e-02,  3.5480e-02,  7.5689e-02, -7.9236e-02,\n",
       "                      -6.0214e-02,  9.0560e-03, -1.5927e-02, -8.9277e-02,  5.8780e-02,\n",
       "                       7.7411e-02, -8.5189e-02, -5.3128e-02, -1.0684e-01, -6.6898e-02,\n",
       "                      -6.4534e-02,  3.8698e-02, -6.0634e-02, -7.5774e-02, -8.3399e-02,\n",
       "                       2.3500e-02,  8.1448e-02, -5.2753e-02, -2.6019e-02,  2.8624e-02,\n",
       "                       4.8589e-02, -1.3256e-03, -2.2555e-02,  3.2608e-02, -3.7026e-02,\n",
       "                      -4.5086e-02, -5.6037e-02,  4.3996e-02, -1.0206e-01,  8.1639e-02,\n",
       "                       3.3230e-02, -1.6550e-02,  4.4231e-02,  4.8543e-05, -6.7316e-02,\n",
       "                       4.2360e-02,  4.8726e-02,  1.9703e-02,  5.8714e-02, -8.2195e-02,\n",
       "                       7.5171e-02,  1.4569e-02,  3.3965e-02,  1.2165e-02,  6.0322e-02,\n",
       "                      -6.5812e-02, -8.0462e-02, -7.2602e-02,  6.6592e-02,  1.6911e-02,\n",
       "                      -6.4489e-02,  3.9592e-02,  6.3424e-02,  7.5393e-02, -7.6540e-02,\n",
       "                       4.3501e-02,  4.9272e-02,  4.1621e-02,  6.3235e-02, -5.0894e-02,\n",
       "                       2.7081e-02, -7.3019e-02, -7.9038e-02,  6.6664e-02,  7.3422e-02,\n",
       "                      -1.0354e-01, -9.3685e-02, -6.0300e-02,  2.1117e-02, -7.2371e-02,\n",
       "                       6.4719e-02, -7.1070e-02, -7.8298e-02, -7.1371e-02,  1.2443e-01])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 3.5625e-02,  7.4333e-02, -5.6597e-02,  4.7155e-02,  8.9672e-03,\n",
       "                       -1.8612e-02, -1.5808e-02, -9.1995e-02,  2.4899e-02, -1.2226e-02,\n",
       "                        1.6893e-02,  1.8687e-02, -7.9355e-02,  3.7671e-03, -9.4376e-02,\n",
       "                        3.2062e-02, -2.0542e-02,  8.3804e-02, -1.0306e-02, -5.1104e-02,\n",
       "                        1.0290e-01,  3.0428e-02,  1.7814e-02, -7.8418e-02,  7.2983e-02,\n",
       "                        5.0244e-02,  1.4215e-02,  1.2368e-01,  4.0672e-02, -1.3202e-01,\n",
       "                        1.1332e-01, -7.0656e-02,  1.0584e-01,  9.0987e-02, -4.9351e-02,\n",
       "                        8.4043e-02, -1.4434e-02, -9.4531e-02,  7.7086e-02, -2.3581e-02,\n",
       "                       -5.6344e-02, -6.2145e-02,  1.3901e-01, -4.9936e-02,  5.5176e-02,\n",
       "                        3.0660e-02, -6.9688e-03,  6.1954e-02,  1.2208e-01, -8.3533e-03,\n",
       "                       -7.8057e-02,  3.6800e-02,  4.7932e-03, -8.2995e-02,  9.4121e-03,\n",
       "                        9.8503e-02,  9.8699e-02,  5.7002e-02,  4.0200e-03, -1.4469e-01,\n",
       "                        8.7430e-02, -3.7615e-03,  4.9216e-02, -7.7330e-02, -7.8997e-02,\n",
       "                        5.3023e-03, -5.0521e-02,  1.1444e-01,  1.1120e-01,  2.5356e-02,\n",
       "                        4.2727e-02, -2.6914e-02, -1.0038e-01,  1.0875e-01,  5.2555e-02,\n",
       "                        6.9055e-02,  8.3724e-03,  5.5647e-02,  4.6658e-02, -7.4303e-02,\n",
       "                       -7.6671e-03, -3.9532e-02,  2.4590e-02,  7.6136e-02, -8.5670e-02,\n",
       "                        9.3424e-02,  2.6548e-02, -2.0439e-02,  5.5652e-02,  2.0625e-02,\n",
       "                        6.3673e-02, -9.3322e-02, -4.4362e-02, -3.8765e-02, -1.4747e-01,\n",
       "                        7.5765e-02, -7.1402e-02,  5.7655e-02, -1.7057e-02,  1.1120e-01,\n",
       "                        4.4659e-02,  5.1321e-02, -8.8882e-03, -8.2016e-02,  8.8596e-02,\n",
       "                        1.3003e-02, -1.4053e-02, -3.9246e-02,  2.4776e-02,  5.4416e-02,\n",
       "                        5.6011e-02, -5.8867e-02,  2.6843e-02,  1.1038e-01, -6.1119e-02,\n",
       "                        1.8183e-02,  2.7438e-02,  9.8700e-02, -1.1672e-01, -4.1317e-02,\n",
       "                        7.1870e-02, -3.4105e-02, -2.5692e-03, -9.8824e-02,  2.2087e-02,\n",
       "                        1.9471e-02,  4.1936e-02, -8.9040e-02,  8.2209e-02,  7.3094e-02,\n",
       "                        9.2735e-02,  8.9382e-02,  8.6555e-02,  8.6461e-02, -5.3065e-02,\n",
       "                       -6.5444e-02,  3.4220e-02, -7.1271e-02, -7.4523e-02, -1.2560e-01,\n",
       "                       -3.5068e-03,  8.5889e-02,  2.5801e-02,  1.1248e-01,  6.8779e-02,\n",
       "                       -7.2613e-02,  4.0911e-02, -8.1322e-02,  8.4469e-02,  7.5615e-02,\n",
       "                       -8.0296e-02, -4.1540e-02, -1.1562e-01,  1.2761e-02,  2.0950e-02,\n",
       "                       -7.6238e-02, -7.5427e-02,  1.1929e-02,  7.4782e-02, -2.2114e-02,\n",
       "                        8.5086e-02, -7.3213e-02,  1.0520e-01, -8.9133e-02,  3.1283e-02,\n",
       "                        4.8684e-02,  8.1134e-02, -2.5206e-02,  1.0459e-01,  4.6722e-02,\n",
       "                       -3.1724e-02,  8.1398e-02, -7.3012e-02,  1.0373e-01,  3.9103e-02,\n",
       "                       -2.5097e-02,  6.9703e-02, -2.6142e-02, -1.6399e-02,  1.1739e-01,\n",
       "                       -7.7956e-02, -3.9182e-02, -6.3767e-02,  8.0601e-02, -1.2688e-01,\n",
       "                        1.0459e-01, -5.3179e-02, -1.2878e-01, -7.9555e-02, -8.4781e-02,\n",
       "                       -7.5210e-02,  6.4663e-02,  5.8330e-02,  1.0360e-01,  7.6802e-02,\n",
       "                        7.1934e-02,  7.6848e-02, -3.4922e-02,  1.0198e-01, -6.2668e-02,\n",
       "                       -4.8072e-02,  8.0334e-02,  2.3319e-02,  8.4289e-02,  7.4314e-02,\n",
       "                       -1.3131e-01, -2.1523e-02, -1.2792e-01,  9.5252e-02,  1.1669e-02,\n",
       "                        6.0412e-02,  5.1397e-04,  1.1390e-01,  8.9503e-03, -5.9465e-02,\n",
       "                        5.7172e-02, -4.9901e-03, -4.7188e-02, -1.4001e-01, -7.0610e-02,\n",
       "                       -6.0923e-02,  5.7496e-02,  8.3361e-02, -4.0680e-02,  7.3707e-02,\n",
       "                        9.3108e-03,  2.7560e-02,  6.0395e-02, -1.0826e-01,  1.2860e-02,\n",
       "                       -1.0051e-01, -1.3665e-01,  7.6376e-02,  8.0120e-02,  1.4399e-02,\n",
       "                       -6.6513e-02, -8.5375e-02,  5.9798e-02,  9.9599e-02,  4.0021e-02,\n",
       "                        5.5663e-02, -5.4581e-02,  8.8652e-02,  2.8391e-02,  9.2814e-02,\n",
       "                        6.8334e-02,  5.4112e-02, -1.1668e-02, -8.6005e-02, -8.5825e-02,\n",
       "                       -1.1689e-01,  5.3731e-02, -6.5511e-02, -7.1953e-02, -6.9010e-02,\n",
       "                       -5.0556e-02,  5.3670e-02, -7.9620e-02,  9.7544e-02,  6.3147e-02,\n",
       "                        6.1520e-02,  1.0805e-01,  2.1056e-02,  3.4609e-02,  9.4837e-02,\n",
       "                       -4.1406e-02,  6.6205e-02,  8.0563e-02,  2.9365e-02,  7.7169e-02,\n",
       "                       -1.9723e-02, -1.2096e-03,  3.6218e-02,  3.2965e-02, -5.1101e-02,\n",
       "                        7.0787e-02, -4.0812e-02,  7.1840e-02,  9.0550e-02, -6.2364e-02,\n",
       "                        1.0027e-01, -2.9561e-02, -1.1611e-02,  6.1569e-02,  1.1333e-01,\n",
       "                        9.0057e-02, -5.1457e-02,  7.6382e-02,  4.0315e-02,  6.6803e-02,\n",
       "                        1.0471e-01,  9.9405e-02, -7.9336e-02, -3.8557e-02, -1.3810e-02,\n",
       "                        1.2380e-02, -1.4914e-03, -6.2138e-02,  9.4333e-02, -2.5177e-02,\n",
       "                       -7.3721e-02,  1.0170e-01,  5.2540e-02, -7.1818e-02,  4.5810e-02,\n",
       "                       -2.3080e-02, -7.8260e-03, -7.9391e-02,  2.6023e-02,  1.3192e-01,\n",
       "                        7.4063e-02, -4.8288e-02,  1.8084e-02,  5.9745e-02, -2.3036e-02,\n",
       "                       -7.4466e-02,  1.0008e-01, -8.1537e-02,  1.0069e-01, -7.1377e-03,\n",
       "                        1.0587e-01,  5.1515e-02,  6.8787e-02, -7.8380e-02,  9.7130e-02,\n",
       "                       -7.9969e-02, -3.9850e-02, -4.1918e-02, -3.7522e-02,  5.2900e-02,\n",
       "                        6.2465e-02,  3.1671e-02,  6.4187e-02,  2.2778e-02, -7.5772e-02,\n",
       "                        6.2884e-02, -3.9765e-02,  9.7968e-02,  7.0732e-02, -3.6722e-02,\n",
       "                       -1.1660e-02, -5.6879e-02,  5.2771e-02,  1.0232e-01, -7.5520e-02,\n",
       "                       -4.6088e-02, -2.9173e-02,  3.7435e-02,  1.0466e-01,  6.4059e-02,\n",
       "                       -6.5962e-02,  3.3019e-03, -8.9798e-02, -8.3068e-02, -1.2310e-01,\n",
       "                       -8.9165e-02,  2.4989e-02,  2.0998e-02, -1.0734e-01,  5.1552e-02,\n",
       "                       -9.2209e-02, -1.1747e-01, -9.8947e-02, -9.4259e-02,  7.8528e-02,\n",
       "                       -2.8311e-02, -4.3458e-02,  3.7659e-02, -1.1726e-01,  5.2316e-02,\n",
       "                        9.7531e-02,  3.6124e-02,  9.2641e-02, -5.4296e-02,  9.5799e-02,\n",
       "                       -1.7159e-02, -9.7234e-02,  1.5722e-02,  3.2912e-02, -8.7798e-02,\n",
       "                        4.3562e-02, -3.0968e-02, -6.4686e-02, -5.8616e-02, -1.4340e-01,\n",
       "                        3.0982e-02,  3.3913e-02,  7.1808e-02, -5.5060e-05,  6.5215e-02,\n",
       "                        3.3927e-02, -2.3157e-02,  8.7600e-03, -5.9130e-02,  2.0045e-02,\n",
       "                        1.0301e-02, -5.2871e-03, -1.0308e-01,  1.2876e-02,  1.0226e-01,\n",
       "                       -1.2038e-01,  4.8108e-02,  1.3751e-02, -9.7980e-02,  6.8170e-02,\n",
       "                        5.5366e-02,  8.1196e-02, -8.6934e-02, -1.1384e-02,  3.4669e-02,\n",
       "                        1.1552e-01,  3.1618e-02, -1.3163e-01,  1.7429e-02, -7.1682e-03,\n",
       "                        8.3212e-02,  1.7041e-02,  5.4604e-02,  1.6944e-02,  2.7986e-02,\n",
       "                        7.4167e-02,  9.4736e-02,  6.5963e-02, -9.7706e-02,  2.4198e-02,\n",
       "                       -4.7596e-02,  8.1369e-02,  4.6056e-02, -6.7091e-02,  1.0063e-01,\n",
       "                       -1.3625e-01, -3.1304e-02, -2.2322e-02,  2.1615e-02, -3.6959e-02,\n",
       "                       -2.9778e-02,  8.3842e-02,  3.2037e-02, -4.6181e-03,  3.5926e-02,\n",
       "                        5.6745e-02, -5.8058e-02,  2.7667e-02, -4.5977e-02,  9.6391e-02,\n",
       "                        9.5273e-02,  2.6909e-02, -2.7925e-02, -3.6136e-02, -2.5068e-02,\n",
       "                        2.8264e-02,  3.7324e-02,  1.0664e-01,  7.6235e-03, -1.0481e-01,\n",
       "                        9.5484e-02, -9.4015e-03,  7.4211e-02,  5.5518e-02,  5.2397e-03,\n",
       "                        5.4185e-02, -9.2124e-02,  6.4670e-02,  7.3629e-02, -5.3680e-02,\n",
       "                        1.3378e-01,  4.0704e-03,  8.7121e-02,  3.6737e-02,  1.0351e-01,\n",
       "                       -3.1783e-02,  4.4298e-02,  7.6205e-02, -7.1143e-02,  1.2761e-01,\n",
       "                        1.8794e-02,  5.0676e-02,  1.0615e-01,  1.0521e-01,  5.5222e-03,\n",
       "                       -3.6199e-02,  1.0638e-01,  3.7815e-02,  1.0693e-01, -6.0099e-02,\n",
       "                       -6.2941e-02, -5.4116e-02,  6.8675e-05, -5.2860e-02, -9.9058e-02,\n",
       "                       -8.4156e-02,  2.5080e-02, -3.6576e-02, -9.7712e-02, -5.0568e-02,\n",
       "                        1.1377e-01,  3.9138e-02, -8.1647e-02,  5.6555e-02, -1.1033e-01],\n",
       "                      [-5.8067e-02, -9.4887e-02,  4.1515e-02, -6.0255e-02, -5.4686e-03,\n",
       "                        3.4171e-02, -4.7132e-02,  9.2998e-02, -4.6893e-02,  5.2809e-02,\n",
       "                       -3.6880e-02, -7.9271e-02,  2.8248e-02, -2.9218e-02,  1.1376e-01,\n",
       "                       -2.3316e-02,  3.7562e-02, -9.8692e-02, -1.1263e-02,  4.9013e-02,\n",
       "                       -2.2665e-02, -2.2931e-02, -9.1765e-02,  6.2320e-02, -9.3294e-02,\n",
       "                       -1.0515e-01, -2.6461e-02, -1.2357e-01, -2.2560e-02,  5.4532e-02,\n",
       "                       -9.6831e-02,  1.8219e-02, -9.7557e-02, -9.1525e-02,  6.7617e-02,\n",
       "                       -2.5932e-02, -3.8820e-02,  1.0034e-01, -8.3139e-02,  7.5596e-02,\n",
       "                        2.7554e-02,  1.1126e-01, -8.2501e-02,  4.2852e-02, -2.3591e-02,\n",
       "                        3.6965e-02, -7.7906e-03, -3.9305e-03, -1.1159e-01, -4.8563e-02,\n",
       "                        7.9612e-02, -7.6160e-02, -1.2397e-03,  5.5504e-02, -1.0610e-02,\n",
       "                       -8.0421e-02, -7.3327e-02, -5.5488e-02,  1.6112e-02,  1.2639e-01,\n",
       "                       -4.8046e-02,  5.8620e-02, -5.3873e-02,  1.2985e-01,  4.4475e-02,\n",
       "                       -2.2997e-02,  2.9555e-02, -8.1037e-02, -8.2172e-02, -9.2632e-02,\n",
       "                        2.9381e-02, -8.2133e-03,  8.2674e-02, -1.0691e-01, -1.8985e-02,\n",
       "                       -3.4836e-02, -4.9469e-02, -9.9594e-02, -4.2983e-02,  8.6549e-02,\n",
       "                        2.9169e-02,  3.7788e-02, -7.1312e-02, -7.1613e-02,  8.1646e-02,\n",
       "                       -8.4955e-03, -3.1596e-03, -2.2950e-02, -4.0087e-02, -7.6779e-02,\n",
       "                       -1.0062e-01,  8.5735e-02,  4.9756e-03,  4.1760e-02,  1.2003e-01,\n",
       "                       -6.1013e-02,  1.6509e-02, -2.8361e-02, -6.7417e-02, -9.2787e-02,\n",
       "                       -7.9938e-02, -3.8115e-02,  1.1529e-02,  2.8759e-02, -5.3060e-02,\n",
       "                       -8.3391e-02, -2.9963e-02,  7.6762e-03, -1.0837e-01, -4.0198e-02,\n",
       "                       -1.2984e-02,  2.4620e-02, -1.7526e-02, -7.6794e-02,  9.5519e-02,\n",
       "                        3.0101e-02, -7.1005e-03, -1.0169e-01,  1.0862e-01,  7.2935e-02,\n",
       "                       -7.2738e-02,  1.3909e-02,  8.2383e-02,  4.4296e-02, -2.8208e-04,\n",
       "                       -5.8378e-02, -9.6921e-02,  7.2389e-02, -6.7036e-02, -7.1168e-02,\n",
       "                       -7.7376e-02, -2.3046e-02, -1.1030e-01, -2.3398e-02,  5.3561e-02,\n",
       "                        7.7141e-02,  1.9244e-02,  1.0502e-02,  1.2329e-02,  9.2554e-02,\n",
       "                        1.6977e-02, -9.7189e-02, -3.2834e-02, -1.0674e-01, -1.5057e-02,\n",
       "                       -9.2660e-05, -7.8817e-02,  9.0293e-02, -1.0956e-01, -6.4941e-02,\n",
       "                        1.2877e-01,  2.9050e-02,  1.2267e-01,  2.4485e-03,  1.0469e-02,\n",
       "                        4.9962e-02,  1.0107e-01,  1.4538e-03, -7.7451e-02,  2.7404e-02,\n",
       "                       -8.7579e-02,  1.0426e-01, -7.0462e-02,  6.1878e-03, -4.5799e-02,\n",
       "                       -4.6408e-02, -3.6613e-02,  2.8236e-02, -8.2137e-02, -3.9379e-02,\n",
       "                        5.9150e-02, -9.7635e-02,  6.6430e-02, -1.0715e-01, -2.6841e-02,\n",
       "                        8.0482e-02, -4.4482e-02,  6.5537e-02, -2.9247e-02, -1.1800e-01,\n",
       "                        8.0129e-02,  6.9814e-02,  5.4600e-02, -2.8378e-02,  9.5747e-02,\n",
       "                       -1.1946e-01,  2.1878e-02,  1.0934e-01,  8.9830e-02,  9.6101e-02,\n",
       "                        1.9461e-02, -3.9384e-02, -3.0779e-02, -7.0913e-02, -8.8649e-02,\n",
       "                       -8.2111e-02, -6.5481e-02, -5.2776e-02, -5.3180e-02,  2.8542e-02,\n",
       "                        8.3329e-02, -3.4175e-02, -8.4059e-02, -6.5350e-02, -2.5851e-02,\n",
       "                        6.2604e-02,  8.0117e-03,  1.3372e-01, -7.3227e-02, -9.8518e-03,\n",
       "                       -5.8438e-02, -7.7170e-02, -6.7863e-02, -1.5162e-02,  2.2456e-02,\n",
       "                       -9.4282e-02, -1.1938e-03,  6.7303e-02,  1.2075e-01,  9.8523e-02,\n",
       "                        4.1990e-02, -3.0342e-02, -2.4052e-02, -2.6932e-02, -6.1499e-02,\n",
       "                       -7.7546e-02, -3.3676e-02, -4.8651e-02,  1.1514e-01, -5.6240e-02,\n",
       "                        7.5064e-02,  1.0557e-01, -4.1021e-02, -5.2313e-02, -5.1788e-02,\n",
       "                        1.3010e-01,  1.1349e-01, -3.1203e-02, -2.8581e-02, -5.5036e-02,\n",
       "                       -8.1339e-02,  2.7936e-02, -2.9923e-02, -9.4701e-02, -8.8903e-02,\n",
       "                       -9.8891e-03, -3.8351e-02,  5.9228e-02,  8.7328e-02,  1.1132e-01,\n",
       "                        8.3994e-02,  1.1820e-02,  2.9766e-02,  4.7173e-02,  1.2381e-01,\n",
       "                        7.8817e-02, -5.8545e-03,  9.8853e-02, -6.0123e-02, -7.0964e-02,\n",
       "                       -8.7507e-02, -1.0182e-01, -5.3883e-02, -1.0008e-01, -9.2069e-02,\n",
       "                        6.1018e-02, -1.2017e-01, -9.9388e-02, -4.0923e-02, -3.9911e-02,\n",
       "                       -3.7149e-02, -3.0496e-02, -4.0800e-02, -2.5649e-02,  5.7674e-02,\n",
       "                       -5.0265e-02, -2.9840e-02, -6.6135e-02, -1.0149e-01,  7.3559e-02,\n",
       "                       -5.7589e-02, -3.8558e-02, -6.3638e-02, -5.8341e-03, -6.0595e-02,\n",
       "                       -5.4918e-02,  6.5600e-02, -4.0796e-02,  5.3096e-03, -9.2222e-02,\n",
       "                       -8.8374e-02, -8.7153e-02,  4.8143e-02,  1.0539e-02,  6.3945e-02,\n",
       "                        4.8148e-02,  5.3008e-02,  3.5731e-02, -7.0181e-02, -4.9009e-04,\n",
       "                        4.9560e-02, -5.9031e-02, -7.0635e-02,  1.7044e-02, -3.4560e-02,\n",
       "                       -1.1170e-02,  8.6811e-02,  1.4166e-01, -8.7760e-02, -7.8216e-02,\n",
       "                       -1.1018e-01, -4.4334e-03, -1.7866e-02, -9.3902e-02, -3.3308e-02,\n",
       "                        1.1201e-01, -9.9722e-02,  7.4935e-02, -1.0825e-01,  3.9378e-03,\n",
       "                       -6.6339e-02, -4.4455e-02, -3.8335e-02,  1.2059e-01, -1.0266e-01,\n",
       "                        5.1541e-02,  3.7088e-02,  9.8979e-02,  5.5231e-02, -1.1677e-01,\n",
       "                       -2.7613e-02, -2.1282e-02, -9.5751e-02, -9.4305e-02,  1.0378e-01,\n",
       "                       -3.1719e-02,  6.4010e-03, -3.6660e-02, -1.0852e-01,  3.1845e-02,\n",
       "                        4.1202e-02,  2.1561e-02, -1.4796e-02, -8.0203e-02,  3.2643e-02,\n",
       "                        3.7036e-02,  5.2957e-02, -9.5727e-02, -4.0204e-02, -9.1398e-03,\n",
       "                        7.1769e-03,  5.6155e-02,  8.8342e-02,  9.8191e-02,  7.9147e-02,\n",
       "                        6.1714e-02, -3.0868e-02, -4.2560e-02,  1.0643e-01, -5.0160e-02,\n",
       "                        3.7952e-02,  6.3675e-02,  9.8989e-02,  3.3604e-02, -1.1151e-01,\n",
       "                        2.7195e-02,  4.0553e-02, -5.2713e-02,  1.1105e-01, -5.3040e-02,\n",
       "                       -1.0932e-01, -2.7135e-02, -5.4287e-02,  4.4808e-02, -1.0151e-01,\n",
       "                        6.1448e-02,  8.9996e-02, -3.1710e-02, -6.0965e-02,  1.1343e-02,\n",
       "                       -2.8543e-02,  7.4416e-02,  1.1037e-01,  7.9389e-02,  8.6060e-02,\n",
       "                       -7.1971e-02,  3.3033e-03, -1.1298e-01, -9.4661e-03, -3.0511e-02,\n",
       "                       -1.0150e-01, -2.4774e-02, -1.2857e-02,  1.1263e-01, -7.0721e-02,\n",
       "                        2.8265e-02,  3.7903e-04,  8.1379e-02,  1.1319e-02, -9.5846e-02,\n",
       "                        9.7651e-02, -9.7204e-02, -5.2686e-02,  5.1456e-02, -4.7024e-02,\n",
       "                       -4.9846e-02, -6.7525e-02,  1.0155e-01,  5.2991e-02, -6.5054e-02,\n",
       "                       -8.2285e-02, -2.8058e-02,  8.8191e-02, -2.5004e-02, -4.4935e-02,\n",
       "                       -6.8086e-02, -8.9769e-02, -7.8462e-02, -1.7156e-02, -9.7395e-02,\n",
       "                       -2.5435e-02, -7.8927e-02, -2.8043e-02,  1.2264e-01, -2.9216e-02,\n",
       "                        1.4185e-02, -7.4316e-02, -7.4193e-02,  3.1618e-02, -9.0745e-02,\n",
       "                        1.2679e-01,  1.5485e-02, -2.2939e-02,  2.9872e-02,  1.5478e-02,\n",
       "                       -2.2862e-02, -9.0435e-02, -3.6971e-02,  4.8568e-02, -1.8469e-02,\n",
       "                       -1.2261e-01,  7.9874e-02, -2.8679e-02,  7.7622e-02, -3.3751e-02,\n",
       "                       -5.0306e-02, -3.8117e-03,  4.6131e-02,  5.6932e-02, -3.1496e-02,\n",
       "                       -5.8081e-02, -1.5560e-03, -1.1159e-01,  1.3732e-02,  1.1574e-01,\n",
       "                       -4.9059e-02,  3.1066e-02, -1.0701e-01, -2.8368e-02, -5.4787e-02,\n",
       "                       -8.1079e-02,  5.3562e-02, -6.0681e-02, -7.7725e-02,  2.1635e-02,\n",
       "                       -8.3953e-02, -6.5541e-02, -6.5595e-02, -8.7099e-02, -6.3895e-02,\n",
       "                        3.2222e-02, -5.6943e-02, -1.9252e-02,  7.3721e-02, -7.9851e-02,\n",
       "                       -2.3237e-02, -9.5541e-02, -5.8300e-02, -8.1470e-02, -4.6489e-02,\n",
       "                        9.8780e-02, -1.0260e-01, -9.7971e-02, -5.9980e-02,  3.9399e-02,\n",
       "                        5.6784e-02,  1.7788e-02, -7.2362e-02,  9.9665e-02,  8.0699e-02,\n",
       "                        2.7605e-02, -6.8307e-02,  3.2224e-02,  9.3975e-02, -2.5959e-02,\n",
       "                       -9.6010e-02, -7.6204e-02,  6.5510e-02, -5.9911e-02,  1.1061e-01]])),\n",
       "             ('fc3.bias', tensor([-0.0791,  0.0613]))])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
