{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89007f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Due to warning when initializing the \"spacy\" tokenizer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow logging\n",
    "logging.getLogger('tensorflow').disabled = True  # disable tensorflow warning messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f59bdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d44f096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>_. . The faster more efficient AMD Zen CPUs in...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>_.. The faster more efficient AMD Zen CPUs in ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>_.. The faster memory efficient AMD Zen CPUs i...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>_.. The faster to more energy efficient AMD Ze...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet sentiment  label\n",
       "0     im getting on borderlands and i will murder yo...  Positive      3\n",
       "1     I am coming to the borders and I will kill you...  Positive      3\n",
       "2     im getting on borderlands and i will kill you ...  Positive      3\n",
       "3     im coming on borderlands and i will murder you...  Positive      3\n",
       "4     im getting on borderlands 2 and i will murder ...  Positive      3\n",
       "...                                                 ...       ...    ...\n",
       "9995  _. . The faster more efficient AMD Zen CPUs in...  Positive      3\n",
       "9996  _.. The faster more efficient AMD Zen CPUs in ...  Positive      3\n",
       "9997  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  Positive      3\n",
       "9998  _.. The faster memory efficient AMD Zen CPUs i...  Positive      3\n",
       "9999  _.. The faster to more energy efficient AMD Ze...  Positive      3\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('artifacts/train_cleaned.csv')\n",
    "\n",
    "# limit df\n",
    "df = df[:10000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380244aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accae582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cd46fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet   sentiment  label\n",
       "0    I mentioned on Facebook that I was struggling ...  Irrelevant      0\n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...     Neutral      2\n",
       "2    @Microsoft Why do I pay for WORD when it funct...    Negative      1\n",
       "3    CSGO matchmaking is so full of closet hacking,...    Negative      1\n",
       "4    Now the President is slapping Americans in the...     Neutral      2\n",
       "..                                                 ...         ...    ...\n",
       "995  ⭐️ Toronto is the arts and culture capital of ...  Irrelevant      0\n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...  Irrelevant      0\n",
       "997  Today sucked so it’s time to drink wine n play...    Positive      3\n",
       "998  Bought a fraction of Microsoft today. Small wins.    Positive      3\n",
       "999  Johnson & Johnson to stop selling talc baby po...     Neutral      2\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.read_csv('artifacts/valid_cleaned.csv')\n",
    "\n",
    "df_valid = df_valid[:1000]\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150373c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab86e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf9296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "def token_gen(text):\n",
    "    \"\"\"\n",
    "    Tokenizes each sentence in a given text and yields the resulting tokens.\n",
    "\n",
    "    Args:\n",
    "        text (list[str]): A list of sentences to tokenize.\n",
    "\n",
    "    Yields:\n",
    "        list[str]: The resulting tokens from each sentence.\n",
    "    \"\"\"\n",
    "    for sent in text:\n",
    "        tokens = tokenizer(sent)\n",
    "        yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8f7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 2 hyper-parameters\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_LENGTH = 100 # to restrict length every sequence \n",
    "\n",
    "vocab = build_vocab_from_iterator(token_gen(df['tweet']),specials=[\"<UNK>\"],max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])  ## to handel OOV problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89492619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9d07bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type is: <class 'generator'>\n",
      "[44, 157, 238, 14, 99, 7, 44, 84, 2081, 19, 41, 5]\n",
      "[3, 130, 405, 6, 2, 0, 7, 3, 84, 754, 19, 41, 5]\n",
      "[44, 157, 238, 14, 99, 7, 44, 84, 754, 19, 41, 5]\n",
      "[44, 157, 405, 14, 99, 7, 44, 84, 2081, 19, 41, 5]\n",
      "[44, 157, 238, 14, 99, 53, 7, 44, 84, 2081, 19, 38, 41, 5]\n",
      "[44, 157, 238, 225, 99, 7, 44, 87, 2081, 19, 41, 5]\n",
      "[140, 3, 828, 8, 413, 288, 486, 236, 9, 100, 1, 1, 1, 162, 19, 45, 40, 173, 3, 130, 8, 2160, 197, 628, 7, 1231, 11, 70, 10, 24, 240, 431, 1, 140, 3, 797, 6, 182, 433, 8, 0, 9, 24, 292, 1, 1, 635, 11, 2, 933, 1884, 0, 2, 0, 3, 211, 832, 1420, 4, 0]\n",
      "[140, 3, 828, 8, 1188, 10, 288, 385, 236, 9, 100, 16, 162, 19, 45, 40, 173, 23, 3, 55, 8, 524, 25, 32, 628, 7, 1231, 11, 70, 10, 24, 240, 431, 5, 3, 797, 6, 182, 8, 0, 9, 24, 292, 20, 635, 29, 2, 933, 2702, 955, 6, 2, 0, 3, 211, 832, 498, 100, 4, 321, 12, 0]\n",
      "[140, 3, 828, 8, 413, 288, 385, 236, 9, 100, 16, 162, 19, 45, 40, 173, 3, 55, 8, 2160, 25, 32, 628, 7, 1231, 11, 70, 10, 24, 240, 431, 1]\n",
      "[140, 3, 828, 8, 413, 288, 486, 236, 9, 100, 1, 1, 1, 162, 19, 45, 40, 173, 3, 130, 8, 2160, 183, 628, 7, 1231, 11, 70, 10, 24, 240, 431, 1, 140, 3, 797, 6, 182, 433, 8, 0, 9, 24, 292, 1, 1, 635, 11, 2, 933, 1884, 0, 2, 0, 3, 211, 832, 1420, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# numericalize tokens from iterator using vocab\n",
    "sequence = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df['tweet']))\n",
    "\n",
    "print('data type is:',type(sequence))\n",
    "\n",
    "count=0\n",
    "for ids in sequence:\n",
    "    print([num for num in ids])\n",
    "    count+=1\n",
    "    if count==10:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4f33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f685b996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1753, 44, 31, 1753, 3857, 881, 31, 8, 1022, 846, 31, 576, 3857, 379]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check how \"numericalize_tokens_from_iterator\" works\n",
    "\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "\n",
    "sequence = numericalize_tokens_from_iterator(vocab,[\"hi how are you\", \"what is your name?\"])\n",
    "list(next(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e07fff-eb78-46b6-a583-fb0d21592c77",
   "metadata": {},
   "source": [
    "### option 1\n",
    "\n",
    "```\n",
    "token_ids = []\n",
    "for i in range(len(df)):\n",
    "    token_id = vocab(tokenizer(df['tweet'][i]))\n",
    "    token_ids.append(token_id)\n",
    "    \n",
    "padded_text = pad_sequence([torch.tensor(x) for x in token_ids], batch_first=True, padding_value=0)\n",
    "\n",
    "padded_text = padded_text[:,:MAX_LENGTH]\n",
    "padded_text\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f4d9155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 100])\n",
      "tensor([[ 44, 157, 238,  ...,   0,   0,   0],\n",
      "        [  3, 130, 405,  ...,   0,   0,   0],\n",
      "        [ 44, 157, 238,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [ 51,  51,  51,  ...,  51,  51,  51],\n",
      "        [ 51,  20,  39,  ...,   0,   0,   0],\n",
      "        [ 51,  20,  39,  ...,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "### option 2\n",
    "\n",
    "# numericalize tokens from iterator using vocab\n",
    "sequence = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df['tweet']))\n",
    "\n",
    "# create a list to store tokenized sequences\n",
    "token_ids = []\n",
    "for i in range(len(df)):\n",
    "    x = list(next(sequence))\n",
    "    token_ids.append(x)\n",
    "\n",
    "# valid_token_ids = torch.tensor(valid_token_ids) # this will throw an error, because all sequence are not of same length\n",
    "\n",
    "# Pad the sequences to the same length along dimension 0\n",
    "padded_text = pad_sequence([torch.tensor(x) for x in token_ids], batch_first=True, padding_value=0)\n",
    "\n",
    "# restrict the length of every sequence upto MAX_LENGTH\n",
    "padded_text = padded_text[:,:MAX_LENGTH]\n",
    "\n",
    "print(padded_text.shape)\n",
    "print(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2a132-5b3b-42bf-a67a-e4e684a19895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97111859-673e-4ff5-b3fb-31e368bb7760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28b2d687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  44,  157,  238,   14,   99,    7,   44,   84, 2081,   19,   41,    5,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b971650-7761-4472-b165-9bbc50efd39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'will', 'see', 'you', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('I will see you!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec5c506c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 84, 62, 19, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vocab([tokens])\n",
    "\n",
    "vocab(tokenizer('I will see you!')) ## similar to the 'fit_on_texts()' in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b4fa961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 84, 62, 19])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(vocab(tokenizer('I will see you')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d703b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf35b463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(vocab.get_stoi())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff74e03-dfec-4371-92cc-961b71ffff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?torch.nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e397feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = torch.nn.Embedding(num_embeddings=len(vocab),embedding_dim=5,padding_idx=0) \n",
    "\n",
    "## if we want to add embedding for a text, we should assign the value to \"num_embeddings\" according to the \n",
    "## max 'integer_id' that is present in the tokenized text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "058b61c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0644, -1.0537,  1.1503,  0.1034,  0.4832],\n",
       "        [ 0.1515, -0.2933,  0.9012,  2.1108, -0.2869],\n",
       "        [ 0.0197, -0.5134, -0.7790,  0.0749,  1.8912],\n",
       "        [-0.0463,  0.0680,  0.9424,  0.1346,  0.2031],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-2.1288,  0.6700,  0.9539, -1.2126, -1.2233]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = embedd(torch.tensor(vocab(tokenizer('I will see you nonsense!'))))\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b2f76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd36066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 100])\n",
      "tensor([[[ 0.0837,  1.5943, -0.0845,  0.4248, -1.9317],\n",
      "         [-1.6009,  0.1027,  1.2657, -2.6765,  0.8783],\n",
      "         [ 0.2927, -0.3056,  0.0113,  0.9458,  0.6206],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3422, -0.6274,  0.7259, -0.8395, -0.8527],\n",
      "         [ 0.8267,  0.3812, -0.2818,  0.1058, -0.1183],\n",
      "         [-0.3243,  0.8092,  1.0785,  0.5918, -0.5983],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0837,  1.5943, -0.0845,  0.4248, -1.9317],\n",
      "         [-1.6009,  0.1027,  1.2657, -2.6765,  0.8783],\n",
      "         [ 0.2927, -0.3056,  0.0113,  0.9458,  0.6206],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1336,  1.0185,  0.6428,  1.1638,  0.5468],\n",
      "         [-1.1336,  1.0185,  0.6428,  1.1638,  0.5468],\n",
      "         [-1.1336,  1.0185,  0.6428,  1.1638,  0.5468],\n",
      "         ...,\n",
      "         [-1.1336,  1.0185,  0.6428,  1.1638,  0.5468],\n",
      "         [-1.1336,  1.0185,  0.6428,  1.1638,  0.5468],\n",
      "         [-1.1336,  1.0185,  0.6428,  1.1638,  0.5468]],\n",
      "\n",
      "        [[-1.1336,  1.0185,  0.6428,  1.1638,  0.5468],\n",
      "         [ 2.2924, -0.3781, -1.5925,  1.9888,  1.1370],\n",
      "         [-0.7223,  0.0593, -0.7779,  0.9681, -0.8159],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.1336,  1.0185,  0.6428,  1.1638,  0.5468],\n",
      "         [ 2.2924, -0.3781, -1.5925,  1.9888,  1.1370],\n",
      "         [-0.7223,  0.0593, -0.7779,  0.9681, -0.8159],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create the Embedding module with the correct weight matrix size\n",
    "embedd = torch.nn.Embedding(len(vocab), 5, padding_idx=0)\n",
    "\n",
    "# Check the shape of the padded_text and compare it to the expected input shape of the Embedding module\n",
    "print(padded_text.shape)\n",
    "# should be: torch.Size([batch_size, sequence_length])\n",
    "\n",
    "# Use the Embedding module with the padded_text\n",
    "input_text = embedd(padded_text)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45e68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c24b5092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the padded_text and compare it to the expected input shape of the Embedding module\n",
    "print(padded_text[0].shape)\n",
    "# should be: torch.Size([batch_size, sequence_length])\n",
    "\n",
    "# Use the Embedding module with the padded_text\n",
    "embedd(padded_text[0]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b2d720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>_. . The faster more efficient AMD Zen CPUs in...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>_.. The faster more efficient AMD Zen CPUs in ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>_.. The faster memory efficient AMD Zen CPUs i...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>_.. The faster to more energy efficient AMD Ze...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet sentiment  label\n",
       "0     im getting on borderlands and i will murder yo...  Positive      3\n",
       "1     I am coming to the borders and I will kill you...  Positive      3\n",
       "2     im getting on borderlands and i will kill you ...  Positive      3\n",
       "3     im coming on borderlands and i will murder you...  Positive      3\n",
       "4     im getting on borderlands 2 and i will murder ...  Positive      3\n",
       "...                                                 ...       ...    ...\n",
       "9995  _. . The faster more efficient AMD Zen CPUs in...  Positive      3\n",
       "9996  _.. The faster more efficient AMD Zen CPUs in ...  Positive      3\n",
       "9997  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...  Positive      3\n",
       "9998  _.. The faster memory efficient AMD Zen CPUs i...  Positive      3\n",
       "9999  _.. The faster to more energy efficient AMD Ze...  Positive      3\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "094939e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 44, 157, 238,  ...,   0,   0,   0],\n",
       "        [  3, 130, 405,  ...,   0,   0,   0],\n",
       "        [ 44, 157, 238,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 51,  51,  51,  ...,  51,  51,  51],\n",
       "        [ 51,  20,  39,  ...,   0,   0,   0],\n",
       "        [ 51,  20,  39,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(padded_text.shape)\n",
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f14ff9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label =df['label'].to_list()\n",
    "\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bdefcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label= torch.tensor(label)\n",
    "print(label.shape)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abadbecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12e2c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(label.unique())\n",
    "\n",
    "class RNNClassify(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize the weights of the module\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        #print('embedded shape:',embedded.shape)\n",
    "        \n",
    "        # Pass the embedded input through the RNN layer\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #print('rnn output shape:',output.shape)\n",
    "        #print('rnn hidden shape:',hidden.shape)\n",
    "        \n",
    "        output = output[:, -1, :]  # taking last output of RNN\n",
    "        #print('rnn last output shape:',output.shape)\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb5610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be011b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37abeed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassify(vocab_size=VOCAB_SIZE,embed_dim=100,hidden_size=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3b0e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  44,  157,  238,   14,   99,    7,   44,   84, 2081,   19,   41,    5,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb3df556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84db2f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f20cdc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1101, -1.0245, -0.6673, -2.0067]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text[0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f9326e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text[0].unsqueeze(0).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4b953ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 44, 157, 238,  ...,   0,   0,   0],\n",
       "        [  3, 130, 405,  ...,   0,   0,   0],\n",
       "        [ 44, 157, 238,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 51,  51,  51,  ...,  51,  51,  51],\n",
       "        [ 51,  20,  39,  ...,   0,   0,   0],\n",
       "        [ 51,  20,  39,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a5f3-0fa9-4a1f-b8c9-b754d4a0e3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d04bc449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 100])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd4d2b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1101, -1.0245, -0.6673, -2.0067],\n",
       "        [-1.1101, -1.0245, -0.6673, -2.0067],\n",
       "        [-1.1101, -1.0245, -0.6673, -2.0067],\n",
       "        ...,\n",
       "        [ 1.8626, -2.0455, -0.1751, -2.3254],\n",
       "        [-1.1101, -1.0245, -0.6673, -2.0067],\n",
       "        [-1.1101, -1.0245, -0.6673, -2.0067]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text.to(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fdcf82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text.to(device)).shape            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b46fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bac64cb",
   "metadata": {},
   "source": [
    "### will try `Batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445860e4",
   "metadata": {},
   "source": [
    "#### first of all, let me fix `(X_train,y_train)` and `(X_test,y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdef9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,y_train\n",
    "\n",
    "X_train,y_train = padded_text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42811656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet   sentiment  label\n",
       "0    I mentioned on Facebook that I was struggling ...  Irrelevant      0\n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...     Neutral      2\n",
       "2    @Microsoft Why do I pay for WORD when it funct...    Negative      1\n",
       "3    CSGO matchmaking is so full of closet hacking,...    Negative      1\n",
       "4    Now the President is slapping Americans in the...     Neutral      2\n",
       "..                                                 ...         ...    ...\n",
       "995  ⭐️ Toronto is the arts and culture capital of ...  Irrelevant      0\n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...  Irrelevant      0\n",
       "997  Today sucked so it’s time to drink wine n play...    Positive      3\n",
       "998  Bought a fraction of Microsoft today. Small wins.    Positive      3\n",
       "999  Johnson & Johnson to stop selling talc baby po...     Neutral      2\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744513ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c7b97fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca6e90-544d-4291-b4e2-33ee557c2aa1",
   "metadata": {},
   "source": [
    "### option 1\n",
    "```\n",
    "valid_token_ids = []\n",
    "for i in range(len(df_valid)):\n",
    "    token_id = vocab(tokenizer(df_valid['tweet'][i]))\n",
    "    valid_token_ids.append(token_id)\n",
    "    \n",
    "padded_text_valid = pad_sequence([torch.tensor(x) for x in valid_token_ids], batch_first=True, padding_value=0)\n",
    "\n",
    "padded_text_valid = padded_text_valid[:,:MAX_LENGTH]\n",
    "padded_text_valid\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15210060-6563-4a76-a0fa-46acd9f5b58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 70])\n",
      "tensor([[   3,    0,   14,  ...,    0,    0,    0],\n",
      "        [1805, 1074,   21,  ...,    0,    0,    0],\n",
      "        [   0,  259,   45,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 744, 1492,   28,  ...,    0,    0,    0],\n",
      "        [2542,    8,    0,  ...,    0,    0,    0],\n",
      "        [4357,  201, 4357,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "### option 2\n",
    "\n",
    "# numericalize tokens from iterator using vocab for df_valid\n",
    "sequence_valid = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df_valid['tweet']))\n",
    "\n",
    "# create a list to store tokenized sequences\n",
    "valid_token_ids = []\n",
    "for i in range(len(df_valid)):\n",
    "    x = list(next(sequence_valid))\n",
    "    valid_token_ids.append(x)\n",
    "    \n",
    "# valid_token_ids = torch.tensor(valid_token_ids) # this will throw an error, because all sequence are not of same length\n",
    "\n",
    "# Pad the sequences to the same length along dimension 0\n",
    "padded_text_valid = pad_sequence([torch.tensor(x) for x in valid_token_ids], batch_first=True, padding_value=0)\n",
    "\n",
    "# restrict the length of every sequence upto MAX_LENGTH\n",
    "padded_text_valid = padded_text_valid[:,:MAX_LENGTH]\n",
    "\n",
    "print(padded_text_valid.shape)\n",
    "print(padded_text_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28db56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_valid = df_valid['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e354f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_valid = torch.tensor(label_valid)\n",
    "#label_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02ca2a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   3,    0,   14,  ...,    0,    0,    0],\n",
       "         [1805, 1074,   21,  ...,    0,    0,    0],\n",
       "         [   0,  259,   45,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 744, 1492,   28,  ...,    0,    0,    0],\n",
       "         [2542,    8,    0,  ...,    0,    0,    0],\n",
       "         [4357,  201, 4357,  ...,    0,    0,    0]]),\n",
       " tensor([0, 2, 1, 1, 2, 1, 3, 3, 3, 1, 3, 3, 1, 2, 1, 3, 3, 1, 3, 1, 1, 2, 0, 1,\n",
       "         2, 2, 1, 0, 0, 1, 3, 3, 1, 3, 1, 2, 2, 0, 3, 2, 3, 2, 2, 2, 3, 2, 1, 1,\n",
       "         1, 2, 3, 1, 1, 3, 3, 3, 3, 3, 1, 0, 1, 3, 3, 0, 1, 2, 1, 0, 2, 1, 3, 1,\n",
       "         1, 3, 3, 0, 3, 0, 2, 2, 2, 3, 3, 2, 3, 2, 1, 0, 1, 2, 2, 1, 3, 0, 0, 1,\n",
       "         1, 1, 2, 3, 2, 1, 3, 3, 2, 3, 2, 3, 1, 2, 2, 2, 1, 2, 1, 2, 2, 3, 3, 2,\n",
       "         1, 1, 3, 1, 2, 1, 3, 2, 1, 2, 0, 3, 2, 3, 3, 0, 2, 2, 0, 0, 0, 2, 2, 0,\n",
       "         0, 0, 3, 2, 3, 0, 3, 1, 2, 2, 2, 0, 2, 1, 2, 3, 1, 2, 1, 0, 0, 0, 2, 1,\n",
       "         1, 1, 3, 3, 3, 2, 2, 3, 0, 2, 2, 2, 3, 2, 1, 1, 2, 3, 3, 0, 0, 2, 3, 3,\n",
       "         2, 0, 2, 1, 1, 1, 1, 3, 2, 2, 3, 3, 3, 3, 1, 3, 3, 0, 2, 0, 1, 1, 0, 0,\n",
       "         1, 3, 3, 1, 0, 1, 3, 3, 1, 0, 0, 3, 3, 1, 3, 0, 2, 0, 0, 1, 2, 2, 3, 1,\n",
       "         0, 0, 3, 3, 0, 0, 2, 3, 1, 1, 3, 3, 3, 3, 2, 2, 3, 1, 2, 3, 2, 1, 2, 2,\n",
       "         1, 3, 3, 0, 1, 2, 0, 3, 2, 0, 1, 2, 1, 3, 3, 1, 1, 1, 3, 1, 2, 3, 2, 2,\n",
       "         1, 3, 1, 3, 1, 0, 2, 2, 3, 1, 2, 1, 0, 3, 1, 3, 0, 3, 3, 3, 3, 3, 1, 1,\n",
       "         3, 1, 2, 2, 2, 3, 0, 2, 3, 0, 1, 2, 2, 0, 2, 2, 0, 1, 3, 1, 1, 0, 0, 3,\n",
       "         0, 3, 2, 2, 0, 0, 1, 1, 1, 2, 3, 0, 2, 1, 3, 0, 2, 1, 1, 1, 3, 2, 2, 0,\n",
       "         1, 3, 3, 0, 2, 0, 3, 2, 2, 3, 3, 1, 2, 3, 1, 2, 1, 0, 1, 3, 3, 0, 3, 3,\n",
       "         2, 1, 2, 0, 0, 3, 2, 3, 1, 1, 1, 0, 3, 2, 3, 0, 1, 2, 0, 1, 3, 3, 3, 3,\n",
       "         2, 2, 0, 2, 1, 3, 3, 2, 1, 3, 2, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 1, 1, 2,\n",
       "         1, 2, 2, 1, 0, 3, 0, 0, 1, 3, 3, 3, 0, 2, 3, 0, 2, 2, 1, 3, 2, 2, 2, 2,\n",
       "         0, 0, 0, 2, 2, 0, 2, 0, 2, 0, 1, 1, 3, 0, 0, 3, 0, 0, 2, 1, 1, 3, 3, 1,\n",
       "         3, 3, 3, 3, 0, 3, 0, 2, 1, 3, 2, 2, 3, 3, 0, 3, 0, 3, 0, 2, 1, 1, 1, 1,\n",
       "         2, 3, 1, 1, 3, 3, 2, 2, 1, 1, 3, 2, 1, 1, 1, 2, 2, 3, 1, 1, 1, 2, 2, 0,\n",
       "         2, 1, 1, 3, 0, 0, 1, 2, 1, 2, 3, 1, 2, 3, 3, 1, 2, 2, 3, 0, 1, 1, 2, 0,\n",
       "         2, 1, 3, 1, 2, 1, 1, 1, 3, 1, 3, 1, 3, 1, 1, 2, 1, 2, 1, 3, 1, 0, 2, 1,\n",
       "         1, 2, 2, 3, 1, 3, 2, 2, 2, 0, 0, 2, 0, 1, 2, 2, 2, 0, 1, 1, 0, 2, 0, 3,\n",
       "         2, 2, 1, 2, 3, 2, 2, 0, 2, 2, 0, 2, 3, 2, 3, 3, 0, 3, 2, 1, 3, 2, 0, 0,\n",
       "         2, 1, 3, 2, 2, 1, 0, 2, 1, 0, 2, 0, 0, 3, 0, 1, 3, 1, 1, 2, 0, 2, 2, 2,\n",
       "         3, 3, 0, 3, 0, 1, 0, 1, 2, 3, 2, 0, 2, 1, 3, 0, 0, 3, 2, 3, 1, 1, 2, 3,\n",
       "         1, 3, 1, 2, 3, 3, 2, 2, 2, 0, 3, 2, 1, 2, 2, 1, 3, 1, 0, 2, 3, 3, 3, 0,\n",
       "         3, 2, 1, 2, 3, 3, 0, 3, 3, 3, 1, 2, 3, 2, 2, 1, 1, 1, 2, 3, 2, 1, 0, 3,\n",
       "         3, 3, 0, 2, 3, 0, 1, 2, 1, 2, 0, 1, 2, 2, 3, 1, 3, 3, 2, 1, 2, 1, 1, 1,\n",
       "         0, 1, 0, 3, 3, 3, 3, 2, 1, 1, 3, 1, 1, 2, 0, 2, 3, 2, 1, 2, 3, 1, 2, 3,\n",
       "         1, 2, 2, 3, 0, 1, 2, 0, 1, 0, 3, 3, 2, 3, 2, 3, 1, 2, 2, 2, 3, 2, 2, 0,\n",
       "         1, 3, 2, 1, 1, 2, 2, 3, 1, 1, 1, 0, 1, 3, 2, 2, 3, 0, 2, 1, 3, 3, 1, 0,\n",
       "         3, 3, 2, 1, 2, 3, 3, 1, 1, 1, 0, 3, 1, 2, 1, 2, 2, 0, 3, 3, 1, 2, 3, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 2, 2, 2, 3, 2, 2, 2, 0, 0, 1, 2, 3, 2,\n",
       "         1, 1, 3, 2, 0, 3, 2, 3, 0, 2, 1, 0, 0, 0, 3, 3, 0, 3, 3, 0, 1, 1, 3, 2,\n",
       "         2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 1, 2, 3, 0, 2, 1, 3, 3, 1, 3, 2, 2, 0, 3,\n",
       "         1, 0, 2, 0, 1, 2, 3, 0, 0, 3, 2, 1, 3, 1, 1, 2, 1, 2, 0, 1, 3, 0, 2, 1,\n",
       "         1, 3, 3, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 0, 3, 2, 3, 3, 3, 1, 2, 3,\n",
       "         2, 0, 0, 3, 2, 3, 2, 0, 3, 2, 1, 1, 3, 3, 3, 0, 3, 2, 0, 1, 3, 3, 1, 1,\n",
       "         1, 2, 3, 0, 2, 3, 3, 2, 3, 1, 3, 0, 0, 3, 3, 2]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test,y_test\n",
    "\n",
    "X_test, y_test = padded_text_valid, label_valid\n",
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c9b8a",
   "metadata": {},
   "source": [
    "#### Now, write the train and test loop for `Batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3f2bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # remember it gives logits (row outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18a91fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 1.5519, Train Accuracy: 0.2618\n",
      "Epoch 1/30, Test Loss: 1.4597, Test Accuracy: 0.2850\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2/30, Train Loss: 1.5052, Train Accuracy: 0.2618\n",
      "Epoch 2/30, Test Loss: 1.4303, Test Accuracy: 0.2840\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3/30, Train Loss: 1.4708, Train Accuracy: 0.2617\n",
      "Epoch 3/30, Test Loss: 1.4078, Test Accuracy: 0.2840\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4/30, Train Loss: 1.4422, Train Accuracy: 0.2617\n",
      "Epoch 4/30, Test Loss: 1.3909, Test Accuracy: 0.2810\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5/30, Train Loss: 1.4183, Train Accuracy: 0.2618\n",
      "Epoch 5/30, Test Loss: 1.3798, Test Accuracy: 0.2800\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6/30, Train Loss: 1.3994, Train Accuracy: 0.2616\n",
      "Epoch 6/30, Test Loss: 1.3745, Test Accuracy: 0.2690\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7/30, Train Loss: 1.3859, Train Accuracy: 0.2271\n",
      "Epoch 7/30, Test Loss: 1.3745, Test Accuracy: 0.2660\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8/30, Train Loss: 1.3777, Train Accuracy: 0.2271\n",
      "Epoch 8/30, Test Loss: 1.3788, Test Accuracy: 0.2760\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9/30, Train Loss: 1.3743, Train Accuracy: 0.3197\n",
      "Epoch 9/30, Test Loss: 1.3864, Test Accuracy: 0.2760\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 10/30, Train Loss: 1.3749, Train Accuracy: 0.3198\n",
      "Epoch 10/30, Test Loss: 1.3955, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 11/30, Train Loss: 1.3780, Train Accuracy: 0.3198\n",
      "Epoch 11/30, Test Loss: 1.4032, Test Accuracy: 0.2760\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 12/30, Train Loss: 1.3813, Train Accuracy: 0.3198\n",
      "Epoch 12/30, Test Loss: 1.4078, Test Accuracy: 0.2760\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 13/30, Train Loss: 1.3832, Train Accuracy: 0.3198\n",
      "Epoch 13/30, Test Loss: 1.4088, Test Accuracy: 0.2770\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 14/30, Train Loss: 1.3830, Train Accuracy: 0.3198\n",
      "Epoch 14/30, Test Loss: 1.4070, Test Accuracy: 0.2770\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 15/30, Train Loss: 1.3811, Train Accuracy: 0.3198\n",
      "Epoch 15/30, Test Loss: 1.4035, Test Accuracy: 0.2770\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 16/30, Train Loss: 1.3784, Train Accuracy: 0.3199\n",
      "Epoch 16/30, Test Loss: 1.3994, Test Accuracy: 0.2770\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 17/30, Train Loss: 1.3757, Train Accuracy: 0.3199\n",
      "Epoch 17/30, Test Loss: 1.3956, Test Accuracy: 0.2770\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 18/30, Train Loss: 1.3736, Train Accuracy: 0.3199\n",
      "Epoch 18/30, Test Loss: 1.3922, Test Accuracy: 0.2770\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 19/30, Train Loss: 1.3722, Train Accuracy: 0.3199\n",
      "Epoch 19/30, Test Loss: 1.3893, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 20/30, Train Loss: 1.3714, Train Accuracy: 0.3199\n",
      "Epoch 20/30, Test Loss: 1.3868, Test Accuracy: 0.2770\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 21/30, Train Loss: 1.3709, Train Accuracy: 0.3199\n",
      "Epoch 21/30, Test Loss: 1.3844, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 22/30, Train Loss: 1.3706, Train Accuracy: 0.3200\n",
      "Epoch 22/30, Test Loss: 1.3821, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 23/30, Train Loss: 1.3703, Train Accuracy: 0.3200\n",
      "Epoch 23/30, Test Loss: 1.3799, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 24/30, Train Loss: 1.3700, Train Accuracy: 0.3201\n",
      "Epoch 24/30, Test Loss: 1.3779, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 25/30, Train Loss: 1.3698, Train Accuracy: 0.3202\n",
      "Epoch 25/30, Test Loss: 1.3762, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 26/30, Train Loss: 1.3697, Train Accuracy: 0.3202\n",
      "Epoch 26/30, Test Loss: 1.3749, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 27/30, Train Loss: 1.3696, Train Accuracy: 0.3203\n",
      "Epoch 27/30, Test Loss: 1.3739, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 28/30, Train Loss: 1.3694, Train Accuracy: 0.3204\n",
      "Epoch 28/30, Test Loss: 1.3733, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 29/30, Train Loss: 1.3693, Train Accuracy: 0.3204\n",
      "Epoch 29/30, Test Loss: 1.3730, Test Accuracy: 0.2790\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 30/30, Train Loss: 1.3691, Train Accuracy: 0.3204\n",
      "Epoch 30/30, Test Loss: 1.3730, Test Accuracy: 0.2790\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Here, we will use Batch Gradient Descent, BUT, generally we prefer Mini-Batch Gradient Descent\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc = 0,0\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    X_train,y_train = X_train.to(device), y_train.to(device)\n",
    "    \n",
    "    y_logits = model(X_train)\n",
    "    #print('shape of y_logits:',y_logits.shape)\n",
    "\n",
    "    \n",
    "    # Compute loss with one-hot encoded targets\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    \n",
    "    train_loss += loss\n",
    "    train_acc += (y_logits.argmax(1) == y_train).sum().item() / len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        test_loss,test_acc = 0,0\n",
    "    \n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        \n",
    "        y_logits = model(X_test)\n",
    "\n",
    "        # Compute loss with one-hot encoded targets\n",
    "        loss = loss_fn(y_logits, y_test)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "            \n",
    "        # Compute accuracy\n",
    "        test_preds = y_logits.argmax(dim=1)\n",
    "        test_acc += (test_preds == y_test).sum().item() / len(y_test)\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "        \n",
    "        print('--'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935adac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
